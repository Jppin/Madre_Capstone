# utils/rag_utils.py

import os
from dotenv import load_dotenv

# â€” Document Loader
# langchain-community ì—ì„œ ì œê³µí•˜ëŠ” document loaderë“¤ì€ ìœ ì§€í•´ë„ ë˜ì§€ë§Œ,
# ë§Œì•½ ì—¬ê¸°ì„œë„ deprecation ê²½ê³ ë¥¼ í”¼í•˜ê³  ì‹¶ìœ¼ë©´ langchain-openai ë²„ì „ìœ¼ë¡œ í™•ì¸í•´ ë³´ì„¸ìš”. 
from langchain_community.document_loaders import (
    PDFMinerLoader, 
    TextLoader, 
    DirectoryLoader, 
    JSONLoader
)

# â€” Text Splitter (ì•„ì§ langchain.text_splitter ì— ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤)
from langchain.text_splitter import RecursiveCharacterTextSplitter

# â€” Embeddings & ChatModel: langchain-openai íŒ¨í‚¤ì§€ì—ì„œ ê°€ì ¸ì˜¤ê¸° 
from langchain_openai import OpenAIEmbeddings, ChatOpenAI

from langchain_anthropic import ChatAnthropic

# ë³€ê²½ í›„
from langchain_chroma import Chroma

# â€” RetrievalQA ìì²´ëŠ” langchain ìª½ì—ì„œ ê°€ì ¸ì™€ë„ ë¬´ë°©í•©ë‹ˆë‹¤.
from langchain.chains import RetrievalQA


load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("RAG_UTILS: OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
if not ANTHROPIC_API_KEY:
    raise RuntimeError("RAG_UTILS: ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")


def initialize_vectorstore(
    persist_directory: str = "chroma_db",
    collection_name: str = "nutrition_guidelines",
    embedding_model_name: str = "text-embedding-3-small",
    chunk_size: int = 1000,
    chunk_overlap: int = 200
):
    from langchain_community.document_loaders import (
        PDFMinerLoader, TextLoader, DirectoryLoader, JSONLoader
    )
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_openai import OpenAIEmbeddings
    from langchain_chroma import Chroma

    # 1) ë¬¸ì„œ ë¡œë“œ
    loader = DirectoryLoader(
        "data",
        glob="**/*.*",
        loader_cls_mapping={
            ".txt": TextLoader,
            ".pdf": PDFMinerLoader,
            ".json": JSONLoader
        }
    )
    documents = loader.load()

    print(f"[ğŸ“„] ë¬¸ì„œ {len(documents)}ê°œ ë¡œë“œë¨")

    # 2) í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len
    )
    docs = text_splitter.split_documents(documents)

    print(f"[âœ‚ï¸] ì´ {len(docs)}ê°œì˜ chunk ìƒì„±ë¨")
    print("[ğŸ”] ì²« chunk ë¯¸ë¦¬ë³´ê¸°:\n", docs[0].page_content[:300])

    # 3) ì„ë² ë”© ë° ì¸ë±ì‹±
    embeddings = OpenAIEmbeddings(
        openai_api_key=OPENAI_API_KEY,
        model=embedding_model_name
    )

    vectordb = Chroma.from_documents(
        documents=docs,
        embedding=embeddings,
        persist_directory=persist_directory,
        collection_name=collection_name
    )
    vectordb.persist()
    return vectordb



def load_vectorstore(
    persist_directory: str = "chroma_db",
    collection_name: str = "nutrition_guidelines",
    embedding_model_name: str = "text-embedding-3-small"
):
    """
    ì´ë¯¸ ì¸ë±ì‹± ì™„ë£Œëœ Chroma ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•´ì„œ ë°˜í™˜
    """
    embeddings = OpenAIEmbeddings(
        openai_api_key=OPENAI_API_KEY,
        model=embedding_model_name
    )
    vectordb = Chroma(
        embedding_function=embeddings,
        persist_directory=persist_directory,
        collection_name=collection_name
    )
    return vectordb


def build_retrieval_qa_chain(
    vectorstore: Chroma,
    llm_model_name: str = "claude-3-5-sonnet-20241022",
    llm_temperature: float = 0.7,
    MAX_TOKENS: int = 2048
):
    """
    (1) vectorstore.as_retriever() -> retriever ìƒì„±
    (2) ChatOpenAI(llm_model_name, api_key, temperature) -> LLM ê°ì²´ ìƒì„±
    (3) RetrievalQA.from_chain_type(llm, retriever) -> QA ì²´ì¸ ìƒì„± í›„ ë°˜í™˜
    """
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 5, "lambda_mult": 0.8}
    )

    # llm = ChatOpenAI(
    #     model_name=llm_model_name,
    #     openai_api_key=OPENAI_API_KEY,
    #     temperature=llm_temperature
    # )
    llm = ChatAnthropic(
        model_name=llm_model_name,
        anthropic_api_key=ANTHROPIC_API_KEY,
        temperature=llm_temperature,
        max_tokens=MAX_TOKENS
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True
    )
    return qa_chain
